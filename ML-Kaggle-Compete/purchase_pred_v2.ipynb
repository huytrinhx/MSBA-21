{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from mxnet import gluon, np, npx, autograd\n",
    "from mxnet.gluon import nn\n",
    "import d2l\n",
    "import mxnet as mx\n",
    "npx.set_np()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read user and items into a dataframe which is then converted into csv\n",
    "# this part takes a while and is only done ONCE\n",
    "# after creating csv, we can upload that into a dataframe directly\n",
    "\n",
    "#def read_file(f):\n",
    " #   for l in open(f):\n",
    " #       yield eval(l)\n",
    "#df = pd.DataFrame()\n",
    "\n",
    "#for l in read_file(\"train.json\"):\n",
    " #   reviewerID,itemID = l['reviewerID'],l['itemID']\n",
    " #   df = df.append({'reviewerID': reviewerID, 'itemID': itemID}, ignore_index = True)\n",
    "#df.to_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can upload csv straight into dataframe\n",
    "data = pd.read_csv(\"train.csv\")\n",
    "data = data.drop(data.columns[0], axis=1)  # drop the unnamed column\n",
    "# check to see if there are any duplicate users + items\n",
    "len(data[data.duplicated()])\n",
    "# add a column to indicate item was purchased\n",
    "data['Purchased'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to speed things up, working with 1000 rows for now..need to remove this part later\n",
    "data = data.drop(data.index[1000:]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pivot table to show every reviewerID and every itemID\n",
    "# this will allow us identify users who did not purchase an item as well\n",
    "df_matrix = pd.pivot_table(data, values='Purchased', index='reviewerID', columns='itemID')\n",
    "df_matrix = df_matrix.reset_index()\n",
    "# undo pivot table and save it as data\n",
    "data = pd.melt(df_matrix, id_vars=['reviewerID'], value_name='Purchased')  # this takes some time to run\n",
    "data = data.fillna(0)  # replace NaN with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_users = data[\"reviewerID\"].unique().shape[0]\n",
    "num_items = data[\"itemID\"].unique().shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since we need our data to be numeric, extracting numerics\n",
    "# KEEP THIS IN MIND WHEN GENERATING PREDICTIONS FOR TEST SET\n",
    "data['reviewerID'] = data['reviewerID'].str.extract('(\\d+)')\n",
    "data['itemID'] = data['itemID'].str.extract('(\\d+)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data into train and validation set, ensuring equal proportion of labels in both\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "def train_validate_split(data, train_size=0.8, validate_size=0.2):\n",
    "    # first we shuffle and split all data into train and test set with equal label proportions\n",
    "    sss = StratifiedShuffleSplit(n_splits = 1, train_size = train_size)\n",
    "    for train_index, validate_index in sss.split(data, data['Purchased']):\n",
    "        train, validate = data.iloc[train_index, : ], data.iloc[validate_index, : ]\n",
    "    return(train, validate)\n",
    "\n",
    "train, validate = train_validate_split(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_u, train_i, train_p = np.array(train['reviewerID'], dtype = 'float32'), np.array(train['itemID'], dtype = 'float32'), np.array(train['Purchased'], dtype = 'float32')\n",
    "validate_u, validate_i, validate_p = np.array(validate['reviewerID'], dtype = 'float32'), np.array(validate['itemID'], dtype = 'float32'), np.array(validate['Purchased'], dtype = 'float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = gluon.data.ArrayDataset(train_u, train_i, train_p)\n",
    "train_iter = gluon.data.DataLoader(train_set, shuffle=True, last_batch='rollover',batch_size=256)\n",
    "\n",
    "validate_set = gluon.data.ArrayDataset(validate_u, validate_i, validate_p)\n",
    "validate_iter = gluon.data.DataLoader(validate_set, shuffle=False, last_batch='rollover',batch_size=256)\n",
    "\n",
    "class MF_user_item_bias(nn.Block):\n",
    "    def __init__(self, num_factors, num_users, num_items, **kwargs):\n",
    "        super(MF_user_item_bias, self).__init__(**kwargs)\n",
    "        self.P = nn.Embedding(input_dim=num_users, output_dim=num_factors)\n",
    "        self.Q = nn.Embedding(input_dim=num_items, output_dim=num_factors)\n",
    "        self.user_bias = nn.Embedding(num_users, 1)\n",
    "        self.item_bias = nn.Embedding(num_items, 1)\n",
    "\n",
    "    def forward(self, user_id, item_id):\n",
    "        P_u = self.P(user_id)\n",
    "        Q_i = self.Q(item_id)\n",
    "        b_u = self.user_bias(user_id)\n",
    "        b_i = self.item_bias(item_id)\n",
    "        outputs = (P_u * Q_i).sum(axis=1) + np.squeeze(b_u) + np.squeeze(b_i)\n",
    "        return outputs.flatten()\n",
    "\n",
    "def evaluator(net, test_iter, ctx):\n",
    "    rmse = mx.metric.RMSE()  # Get the RMSE\n",
    "    rmse_list = []\n",
    "    for idx, (users, items, ratings) in enumerate(test_iter):\n",
    "        u = gluon.utils.split_and_load(users, ctx, even_split=False)\n",
    "        i = gluon.utils.split_and_load(items, ctx, even_split=False)\n",
    "        r_ui = gluon.utils.split_and_load(ratings, ctx, even_split=False)\n",
    "        r_hat = [net(u, i) for u, i in zip(u, i)]\n",
    "        rmse.update(labels=r_ui, preds=r_hat)\n",
    "        rmse_list.append(rmse.get()[1])\n",
    "    return float(np.mean(np.array(rmse_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model for user and item only\n",
    "ctx = d2l.try_all_gpus()\n",
    "ctx_list=d2l.try_all_gpus()\n",
    "net = MF_user_item_bias(30, num_users, num_items)\n",
    "net.initialize(ctx=ctx, force_reinit=True, init=mx.init.Normal(0.01))\n",
    "lr, num_epochs, wd, optimizer = 0.002, 30, 1e-5, 'adam'\n",
    "loss = gluon.loss.L2Loss()\n",
    "trainer = gluon.Trainer(net.collect_params(), optimizer,{\"learning_rate\": lr, 'wd': wd})\n",
    "    \n",
    "for epoch in range(num_epochs):\n",
    "    l = 0\n",
    "    metric = d2l.Accumulator(3)\n",
    "    for i, values in enumerate(train_iter):\n",
    "        input_data = []  # 3 arrays: train_u, train_i, train_p\n",
    "        values = values if isinstance(values, list) else [values]\n",
    "        for v in values:\n",
    "            input_data.append(gluon.utils.split_and_load(v, ctx_list))\n",
    "        train_feat = input_data[0:-1] if len(values) > 1 else input_data   # 2 arrays: train_u, train_i\n",
    "        train_label = input_data[-1]  # train_p\n",
    "        with autograd.record():\n",
    "            preds = [net(*t) for t in zip(*train_feat)]\n",
    "            ls = [loss(p, s) for p, s in zip(preds, train_label)]\n",
    "        [l.backward() for l in ls]\n",
    "        l += sum([l.asnumpy() for l in ls]).mean() / len(ctx_list)\n",
    "        trainer.step(values[0].shape[0])\n",
    "        metric.add(l, values[0].shape[0], values[0].size)\n",
    "    train_l = l / (i + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-219-2772d17e2c8d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mmetric\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0md2l\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/d2l/lib/python3.7/site-packages/d2l/d2l.py\u001b[0m in \u001b[0;36mevaluate_accuracy\u001b[0;34m(net, data_iter)\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_iter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0mmetric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "def evaluate_accuracy(net, data_iter):\n",
    "    metric = Accumulator(2)  # num_corrected_examples, num_examples\n",
    "    for i, values in enumerate(train_iter):\n",
    "        input_data = []  # 3 arrays: train_u, train_i, train_p\n",
    "        values = values if isinstance(values, list) else [values]\n",
    "        for v in values:\n",
    "            input_data.append(gluon.utils.split_and_load(v, ctx_list))\n",
    "        train_feat = input_data[0:-1] if len(values) > 1 else input_data   # 2 arrays: train_u, train_i\n",
    "        train_label = input_data[-1]  # train_p\n",
    "        preds = [net(*t) for t in zip(*train_feat)]\n",
    "        metric.add(d2l.accuracy(np.array(preds), np.array(train_label), np.array(train_label).size))\n",
    "        metric[0] / metric[1]\n",
    "\n",
    "d2l.evaluate_accuracy(net, train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
